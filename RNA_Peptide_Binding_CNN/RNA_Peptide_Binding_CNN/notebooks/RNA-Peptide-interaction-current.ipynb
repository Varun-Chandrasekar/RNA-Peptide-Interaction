{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70a448e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB import PDBList, PDBParser, PPBuilder\n",
    "from Bio.Seq import Seq\n",
    "from Bio.PDB.Polypeptide import is_aa\n",
    "from Bio.SeqUtils import seq1\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbac2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_fasta_file(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    sequence = ''.join([line.strip() for line in lines if not line.startswith(\">\")])\n",
    "    return sequence\n",
    "\n",
    "def parse_fasta_file(filepath):\n",
    "    try:\n",
    "        return [str(record.seq) for record in SeqIO.parse(filepath, \"fasta\")]\n",
    "    except Exception:\n",
    "        return []  # Skip malformed files\n",
    "\n",
    "# Function to gather all .fasta/.fa files recursively\n",
    "def get_fasta_files_from_nested_folders(root_folder, limit=None):\n",
    "    files = glob.glob(os.path.join(root_folder, \"**\", \"*.fa*\"), recursive=True)\n",
    "    return list(islice(files, limit)) if limit else list(files)\n",
    "\n",
    "# Function to load sequences from multiple files using multiprocessing\n",
    "def load_fasta_sequences_parallel(file_list, num_workers=4):\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(parse_fasta_file, file_list)\n",
    "        return list(chain.from_iterable(results))\n",
    "\n",
    "def parse_fasta_sequences(filepath):\n",
    "    peptides = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        current_seq = ''\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                if current_seq:\n",
    "                    peptides.append(current_seq)\n",
    "                    current_seq = ''\n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "        if current_seq:\n",
    "            peptides.append(current_seq)\n",
    "    return peptides\n",
    "\n",
    "fasta_files = get_fasta_files_from_nested_folders(\"RNA_FASTA_Files\", limit=200000)\n",
    "rna_seqs = load_fasta_sequences_parallel(fasta_files, num_workers=8)\n",
    "peptide_seqs = parse_fasta_sequences(\"peptideatlas.fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9dfa7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_multiple_pdbs(pair_file, out_dir):\n",
    "    from Bio.PDB import PDBList\n",
    "    import os\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    pdbl = PDBList()\n",
    "    pdb_ids = set()\n",
    "\n",
    "    with open(pair_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 2 or parts[0].lower() == \"protein\":\n",
    "                continue  # Skip headers or malformed lines\n",
    "            prot, rna = parts\n",
    "            pdb_ids.add(prot.split(\"_\")[0].lower())\n",
    "\n",
    "    for pdb_id in sorted(pdb_ids):\n",
    "        try:\n",
    "            url = f\"https://files.rcsb.org/download/{pdb_id.upper()}.pdb\"\n",
    "            filepath = os.path.join(out_dir, f\"{pdb_id}.pdb\")\n",
    "            if not os.path.exists(filepath):\n",
    "                import urllib.request\n",
    "                urllib.request.urlretrieve(url, filepath)\n",
    "        except Exception as e:\n",
    "          print(f'Nothing')\n",
    "\n",
    "        \n",
    "#download_multiple_pdbs(\"RPI2241.txt\", \"./pdb_files\")\n",
    "\n",
    "def load_rpi2241_pairs(filepath):\n",
    "    pairs = set()\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 2 or parts[0].lower() == \"protein\":\n",
    "                continue  # Skip headers or malformed lines\n",
    "            prot, rna = parts\n",
    "            pairs.add((prot.upper(), rna.upper()))\n",
    "    return pairs\n",
    "\n",
    "rpi2241_positive_pairs=load_rpi2241_pairs(\"RPI2241.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d8f28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding function for RNA and Peptide sequences\n",
    "def one_hot_encodeRNA(sequence):\n",
    "    mapping = {'A': 0, 'U': 1, 'G': 2, 'C': 3}\n",
    "    one_hot = np.zeros((len(sequence), 4))\n",
    "    for i, base in enumerate(sequence):\n",
    "        if base in mapping:\n",
    "            one_hot[i, mapping[base]] = 1\n",
    "    return torch.tensor(one_hot, dtype=torch.float).permute(1, 0)\n",
    "def one_hot_encodepeptide(sequence):\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',\n",
    "    'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    aa_to_index = {aa: idx for idx, aa in enumerate(amino_acids)}\n",
    "    one_hot = np.zeros((len(sequence), 20))\n",
    "    for i, aa in enumerate(sequence):\n",
    "        if aa in aa_to_index:\n",
    "            one_hot[i, aa_to_index[aa]] = 1\n",
    "    return torch.tensor(one_hot, dtype=torch.float).permute(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "121a0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protein_sequence(pdb_file, chain_id):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"protein\", pdb_file)\n",
    "    for model in structure:\n",
    "        if chain_id in model:\n",
    "            chain = model[chain_id]\n",
    "            peptides = PPBuilder().build_peptides(chain)\n",
    "            if peptides:\n",
    "                return str(peptides[0].get_sequence())\n",
    "    return None\n",
    "\n",
    "RNA_MAP = {\n",
    "    \"ADE\": \"A\", \"CYT\": \"C\", \"GUA\": \"G\", \"URI\": \"U\",\n",
    "    \"PSU\": \"U\", \"INO\": \"I\", \"GTP\": \"G\", \"OMC\": \"C\",\n",
    "    \"A\": \"A\", \"C\": \"C\", \"G\": \"G\", \"U\": \"U\"  # Handle both 1/3-letter\n",
    "}\n",
    "\n",
    "def extract_rna_sequence(pdb_file, chain_id):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"rna\", pdb_file)\n",
    "    sequence = \"\"\n",
    "    \n",
    "    # Use first model only\n",
    "    model = structure[0]\n",
    "    if chain_id not in model:\n",
    "        return None\n",
    "    \n",
    "    for residue in model[chain_id]:\n",
    "        # Skip heteroatoms and insertions\n",
    "        residue_id = residue.get_id()\n",
    "        if residue_id[0] != \" \":\n",
    "            continue\n",
    "            \n",
    "        resname = residue.get_resname().strip().upper()\n",
    "        if resname in RNA_MAP:\n",
    "            sequence += RNA_MAP[resname]\n",
    "    \n",
    "    return sequence if sequence else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f63aad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_structure_chains(pdb_files, pdb_dir):\n",
    "    structure_chains = defaultdict(lambda: {'protein': [], 'rna': []})\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_id = os.path.splitext(os.path.basename(pdb_file))[0].lower()\n",
    "        pdb_path = os.path.join(pdb_dir, pdb_file)\n",
    "        try:\n",
    "            structure = parser.get_structure(pdb_id, pdb_path)\n",
    "            model = next(structure.get_models())\n",
    "            for chain in model.get_chains():\n",
    "                chain_id = chain.id.upper()\n",
    "                residues = list(chain.get_residues())\n",
    "                # Count amino acid residues\n",
    "                aa_residues = {\"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLN\", \"GLU\", \"GLY\",\n",
    "                               \"HIS\", \"ILE\", \"LEU\", \"LYS\", \"MET\", \"PHE\", \"PRO\", \"SER\",\n",
    "                               \"THR\", \"TRP\", \"TYR\", \"VAL\"}\n",
    "                rna_residues = {\"ADE\", \"CYT\", \"GUA\", \"URI\", \"PSU\", \"INO\"}\n",
    "                \n",
    "                aa_count = sum(1 for r in residues if r.get_id()[0] == ' ' and r.get_resname() in aa_residues)\n",
    "                rna_count = sum(1 for r in residues if r.get_id()[0] == ' ' and r.get_resname() in rna_residues)\n",
    "                \n",
    "                if aa_count > rna_count:\n",
    "                    structure_chains[pdb_id]['protein'].append(f\"{pdb_id}_{chain_id}\")\n",
    "                elif rna_count > 0:\n",
    "                    structure_chains[pdb_id]['rna'].append(f\"{pdb_id}_{chain_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdb_file}: {e}\")\n",
    "    \n",
    "    return structure_chains\n",
    "    \n",
    "pdb_files = [f for f in os.listdir(\"./pdb_files\") if f.endswith(\".pdb\")]\n",
    "rpi_structure_chains = define_structure_chains(pdb_files,\"./pdb_files\")\n",
    "    \n",
    "def generate_structure_based_negatives(positive_pairs, structure_chains, num_negatives=None, seed=42):\n",
    "    random.seed(seed)\n",
    "    negative_pairs = set()\n",
    "\n",
    "    for pdb_id, chains in structure_chains.items():\n",
    "        protein_chains = chains.get(\"protein\", [])\n",
    "        rna_chains = chains.get(\"rna\", [])\n",
    "        for prot in protein_chains:\n",
    "            for rna in rna_chains:\n",
    "                if (prot, rna) not in positive_pairs:\n",
    "                    negative_pairs.add((prot, rna))\n",
    "                    \n",
    "    return negative_pairs\n",
    "\n",
    "rpi2241_negative_pairs = generate_structure_based_negatives(positive_pairs=rpi2241_positive_pairs, structure_chains=rpi_structure_chains, num_negatives=len(rpi2241_positive_pairs))\n",
    "\n",
    "# Attach labels\n",
    "positive_labeled = [(p, r, 1.0) for p, r in rpi2241_positive_pairs]\n",
    "negative_labeled = [(p, r, 0.0) for p, r in rpi2241_negative_pairs]\n",
    "\n",
    "# Combine and shuffle\n",
    "all_labeled_pairs = positive_labeled + negative_labeled\n",
    "random.shuffle(all_labeled_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a325096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain_sequence(chain):\n",
    "    sequence = \"\"\n",
    "    for residue in chain:\n",
    "        if residue.get_id()[0] != \" \":  # Skip heteroatoms/water\n",
    "            continue\n",
    "        resname = residue.get_resname()\n",
    "        try:\n",
    "            letter = seq1(resname)  # Convert 3-letter to 1-letter\n",
    "            sequence += letter\n",
    "        except Exception:\n",
    "            continue  # Skip unknown residues\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ae9f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAPeptideDataset(Dataset):\n",
    "    def __init__(self, labeled_pairs, pdb_dir):\n",
    "        self.pairs = labeled_pairs\n",
    "        self.pdb_dir = pdb_dir\n",
    "        # Cache to store parsed structures\n",
    "        self.structure_cache = {}\n",
    "        # Cache to store extracted sequences\n",
    "        self.sequence_cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        prot_chain, rna_chain, label = self.pairs[idx]\n",
    "        pdb_id = prot_chain.split(\"_\")[0].lower()\n",
    "        pep_chain_id = prot_chain.split(\"_\")[1].upper()\n",
    "        rna_chain_id = rna_chain.split(\"_\")[1].upper()\n",
    "        \n",
    "        # Get PDB structure (with caching)\n",
    "        pdb_path = os.path.join(self.pdb_dir, f\"{pdb_id}.pdb\")\n",
    "        if not os.path.exists(pdb_path):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Use cached structure if available\n",
    "            if pdb_id not in self.structure_cache:\n",
    "                parser = PDBParser(QUIET=True)\n",
    "                structure = parser.get_structure(pdb_id, pdb_path)\n",
    "                self.structure_cache[pdb_id] = structure\n",
    "            \n",
    "            structure = self.structure_cache[pdb_id]\n",
    "            model = next(structure.get_models())\n",
    "            \n",
    "            # Get chains (with caching)\n",
    "            cache_key = f\"{pdb_id}_{pep_chain_id}_{rna_chain_id}\"\n",
    "            if cache_key not in self.sequence_cache:\n",
    "                pep_chain = model[pep_chain_id]\n",
    "                rna_chain = model[rna_chain_id]\n",
    "                \n",
    "                pep_seq = get_chain_sequence(pep_chain)\n",
    "                rna_seq = get_chain_sequence(rna_chain)\n",
    "                \n",
    "                self.sequence_cache[cache_key] = (pep_seq, rna_seq)\n",
    "            else:\n",
    "                pep_seq, rna_seq = self.sequence_cache[cache_key]\n",
    "            \n",
    "            if not pep_seq or not rna_seq:\n",
    "                return None\n",
    "                \n",
    "            # Convert to tensors\n",
    "            rna_tensor = one_hot_encodeRNA(rna_seq).float()\n",
    "            pep_tensor = one_hot_encodepeptide(pep_seq).float()\n",
    "            label_tensor = torch.tensor(label, dtype=torch.float).view(1)\n",
    "            \n",
    "            return rna_tensor, pep_tensor, label_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdb_id}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    \n",
    "    rna_seqs, pep_seqs, labels = zip(*batch)\n",
    "    \n",
    "    # Get max lengths in THIS batch\n",
    "    max_rna_len = max(seq.shape[0] for seq in rna_seqs)\n",
    "    max_pep_len = max(seq.shape[0] for seq in pep_seqs)\n",
    "    \n",
    "    # Pad to batch-specific max lengths\n",
    "    rna_padded = torch.stack([\n",
    "        F.pad(seq, (0, 0, 0, max_rna_len - seq.shape[0])) \n",
    "        for seq in rna_seqs\n",
    "    ])\n",
    "    \n",
    "    pep_padded = torch.stack([\n",
    "        F.pad(seq, (0, 0, 0, max_pep_len - seq.shape[0]))\n",
    "        for seq in pep_seqs\n",
    "    ])\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    return rna_padded, pep_padded, labels\n",
    "full_dataset = RNAPeptideDataset(labeled_pairs=all_labeled_pairs, pdb_dir=\"./pdb_files\")\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f3c22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.rna_conv1 = nn.Conv1d(4, 16, kernel_size=3, padding=1)\n",
    "        self.rna_conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.rna_conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pep_conv1 = nn.Conv1d(20, 16, kernel_size=3, padding=1)\n",
    "        self.pep_conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pep_conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, rna_input, pep_input):\n",
    "        rna = F.relu(self.rna_conv1(rna_input.permute(0, 2, 1)))\n",
    "        rna = F.relu(self.rna_conv2(rna))\n",
    "        rna = F.relu(self.rna_conv3(rna))\n",
    "        rna = F.adaptive_max_pool1d(rna, 1).squeeze(2)\n",
    "        \n",
    "        pep = F.relu(self.pep_conv1(pep_input.permute(0, 2, 1)))\n",
    "        pep = F.relu(self.pep_conv2(pep))\n",
    "        pep = F.relu(self.pep_conv3(pep))\n",
    "        pep = F.adaptive_max_pool1d(pep, 1).squeeze(2)\n",
    "        \n",
    "        combined = torch.cat((rna, pep), dim=1)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        out = self.fc2(x)  \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9be6349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "model = CNN()\n",
    "pos_weight = len(negative_labeled) / len(positive_labeled)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2066a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        if batch is None or len(batch[0]) == 0:  # Enhanced check\n",
    "            continue\n",
    "        \n",
    "        rna_batch, pep_batch, label_batch = batch\n",
    "        rna_batch = rna_batch.to(device)\n",
    "        pep_batch = pep_batch.to(device)\n",
    "        label_batch = label_batch.float().view(-1,1).to(device)\n",
    "        \n",
    "        # Fix input dimensions\n",
    "        rna_batch = rna_batch.permute(0, 2, 1)  \n",
    "        pep_batch = pep_batch.permute(0, 2, 1)  \n",
    "        \n",
    "        scores = model(rna_batch, pep_batch)\n",
    "        loss = criterion(scores, label_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0 and len(train_losses) > 0:\n",
    "        avg_loss = sum(train_losses) / len(train_losses)\n",
    "        print(f\"Epoch [{epoch + 1}/30], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:  \n",
    "            if batch is None or len(batch[0]) == 0:  # Enhanced check\n",
    "                continue\n",
    "                \n",
    "            rna_batch, pep_batch, label_batch = batch\n",
    "            rna_batch = rna_batch.to(device)\n",
    "            pep_batch = pep_batch.to(device)\n",
    "            label_batch = label_batch.float().view(-1,1).to(device)\n",
    "            \n",
    "            rna_batch = rna_batch.permute(0, 2, 1)  \n",
    "            pep_batch = pep_batch.permute(0, 2, 1) \n",
    "            \n",
    "            val_scores = model(rna_batch, pep_batch)\n",
    "            val_loss = criterion(val_scores, label_batch)\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "    if (epoch + 1) % 10 == 0 and len(val_losses) > 0:      \n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f\"Epoch [{epoch + 1}/30], Val Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16caa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
